{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualização e interpretação dos dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Base de Treino - Análise Exploratória**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando arquivo .csv como um DataFrame pandas\n",
    "arquivo_treino  =  './dataset/fashion-mnist_train.csv'\n",
    "train_fmnist = pd.read_csv(arquivo_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo dimensões do DataFrame\n",
    "train_fmnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as primeiras 5 linhas\n",
    "train_fmnist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo se pelo menos uma coluna possui valores nulos\n",
    "\"\"\"  O primeiro any() retorna se há valores nulos em todas as colunas\n",
    "dada a quantidade de colunas, outro any() é chamado a fim de \n",
    "conferir se pelo menos alguma das colunas é nula \"\"\"\n",
    "is_null = train_fmnist.isna().any().any()\n",
    "print(\"Pelo menos uma coluna possui valores nulos\" if is_null else \"Nenhuma coluna possui valores nulos\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualização dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna de label para manter somente os valores dos pixels\n",
    "pixels = train_fmnist.drop('label', axis=1)\n",
    "\"\"\" pixels.stack() transforma o dataframe em uma série em que todas as colunas vão se condensar\n",
    "em uma única coluna sendo assim o tamanho da série é dado por 60.000 linhas x 784 colunas\n",
    "pixels.stack().value_counts() conta a ocorrência de cada valor de intensidade existente \n",
    "pixels.stack().value_counts().sort_index() ordena a série em termos dos novo índice, isto é, \n",
    "os valores de intensidade de pixel\"\"\"\n",
    "value_counts = pixels.stack().value_counts().sort_index()\n",
    "\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.xlabel('Valor do Pixel')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Histograma da Distribuição de Frequência dos Valores dos Pixels')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - O histograma acima mostra a contagem de ocorrências dos valores de intensidade dos pixels para todas as imagens.  \n",
    "> - Podemos notar que a maioria dos pixels possuem intensidade 0, isto é, boa parte da região das imagens possuem a cor preta. A cor branca, intensidade 255, parece ser a segunda que mais aparece nas imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Distribuição da intensidade média com base no rótulo** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 classes de artigos de moda no total\n",
    "num_classes = 10  \n",
    "# Preenchendo um array de tamanho 10 com 0s\n",
    "mean_intensities = np.zeros(num_classes)\n",
    "\n",
    "# Percorrendo de 0 a 9\n",
    "for i in range(num_classes):\n",
    "    # Obtendo as imagens existentes para cada classe\n",
    "    class_data = train_fmnist[train_fmnist['label'] == i]\n",
    "    \"\"\" Obtendo a média aritmética das intensidades de pixel  \n",
    "    desconsiderando a primeira coluna (coluna de label) \"\"\"\n",
    "    \"\"\" Neste caso é obtida a média de todos os valores \n",
    "    em todo dataframe, independente da linha e coluna \"\"\"\n",
    "    mean_intensity = class_data.iloc[:, 1:].values.mean()\n",
    "    print(mean_intensity)\n",
    "    # Atribuindo o valor médio para o índice da classe no vetor\n",
    "    mean_intensities[i] = mean_intensity\n",
    "\n",
    "plt.bar(range(num_classes), mean_intensities, color='purple')\n",
    "plt.title('Distribuição da Intensidade Média por Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Intensidade Média')\n",
    "plt.xticks(range(num_classes))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "mean_intensities = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_data = train_fmnist[train_fmnist['label'] == i]\n",
    "    \"\"\" Neste caso a média é calculada ao longo do eixo 1, \n",
    "    o que significa que a média é calculada para cada linha do dataframe \n",
    "    gerando um valor médio para cada imagem da classe\"\"\"\n",
    "    mean_intensity = class_data.iloc[:, 1:].values.mean(axis=1)\n",
    "    print(mean_intensity)\n",
    "    # Adicionando o vetor com as médias ao vetor geral\n",
    "    mean_intensities.append(mean_intensity)\n",
    "\n",
    "label_dictionary = {0: \"Camiseta\",\n",
    "                    1: \"Calça\",\n",
    "                    2: \"Pulôver\",\n",
    "                    3: \"Vestido\",\n",
    "                    4: \"Casaco\",\n",
    "                    5: \"Sandália\",\n",
    "                    6: \"Camisa\",\n",
    "                    7: \"Tênis\",\n",
    "                    8: \"Bolsa\",\n",
    "                    9: \"Bota de tornozelo\"}\n",
    "\n",
    "# Criar um gráfico de densidade\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_classes):\n",
    "    # sns.kdeplot(mean_intensities[i], label=str(i))\n",
    "    sns.kdeplot(mean_intensities[i], label=label_dictionary[i])\n",
    "plt.title('Distribuição da Intensidade Média por Rótulo')\n",
    "plt.xlabel('Intensidade Média dos Pixels')\n",
    "plt.ylabel('Densidade')\n",
    "plt.legend(title='Rótulo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - É possível perceber que em geral o padrão das distribuições parecem seguir uma distribuição normal.  \n",
    "> - Aparentemente tênis e calça parecer ter a uma distribuição normal mais consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_variances = []\n",
    "\n",
    "for i in range(10):\n",
    "    class_data = train_fmnist[train_fmnist['label'] == i]\n",
    "    \"\"\" A variância é uma medida de dispersão que indica o quão distantes os \n",
    "    valores de um conjunto estão da média, neste caso estão sendo analisados \n",
    "    todos os valores de pixels para cada classe \"\"\"\n",
    "    class_variance = np.var(class_data.iloc[:, 1:].values) \n",
    "    class_variances.append(class_variance)\n",
    "\n",
    "\"\"\" Obtendo os índices das classes mais e menos variadas\n",
    "argmax e arfmin retornam o índice dos maiores e menores valores\n",
    "calculados, respectivamente \"\"\"\n",
    "most_varied_class = np.argmax(class_variances)\n",
    "least_varied_class = np.argmin(class_variances)\n",
    "\n",
    "# Configurando 1 figura com 2 subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plotando histograma da classe menos variada\n",
    "least_varied_data = train_fmnist[train_fmnist['label'] == least_varied_class].iloc[:, 1:].mean(axis=1)\n",
    "axs[0].hist(least_varied_data, bins=50, color='blueviolet', alpha=0.7)\n",
    "axs[0].set_title(f'Histograma da Classe Menos Variada ({label_dictionary[least_varied_class]})')\n",
    "axs[0].set_xlabel('Intensidade Média de Pixel')\n",
    "axs[0].set_ylabel('Frequência')\n",
    "\n",
    "# Plotando histograma da classe mais variada\n",
    "most_varied_data = train_fmnist[train_fmnist['label'] == most_varied_class].iloc[:, 1:].mean(axis=1)\n",
    "axs[1].hist(most_varied_data, bins=50, color='indigo', alpha=0.7)\n",
    "axs[1].set_title(f'Histograma da Classe Mais Variada ({label_dictionary[most_varied_class]})')\n",
    "axs[1].set_xlabel('Intensidade Média de Pixel')\n",
    "axs[1].set_ylabel('Frequência')\n",
    "\n",
    "# Ajustando o layout para evitar sobreposição\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - O histograma da classe Sandália mostra que os valores dos pixels estão mais concentrados em torno da média, enquanto o histograma de Casaco demonstra que os valores estão mais dispersos em torno da média.  \n",
    "> - A presença de caudasmais longas no histograma dois certifica a presença de algunsvalores de pixels considerados outliers em relação ao conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Criando e Visualizando Exemplos de Imagens no formato PGM ASCII**  \n",
    "As imagens PGM (Portable Gray Map) ASCII são um tipo de formato de arquivo de imagem que armazena imagens em tons de cinza de forma textual legível pelo humano. O formato PGM ASCII é uma variação do formato PGM, que pode armazenar imagens em tons de cinza ou em escala de cinza de forma simples e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Obtendos as imagens para cada classe\n",
    "    class_data = train_fmnist[train_fmnist['label'] == i]\n",
    "    # Armazenando a primeira imagenm como exemplo\n",
    "    img_example = class_data.values[0]\n",
    "    \"\"\" 0: \"Camiseta/top\",\n",
    "    1: \"Calça\",\n",
    "    2: \"Pulôver\",\n",
    "    3: \"Vestido\",\n",
    "    4: \"Casaco\",\n",
    "    5: \"Sandália\",\n",
    "    6: \"Camisa\",\n",
    "    7: \"Tênis\",\n",
    "    8: \"Bolsa\",\n",
    "    9: \"Bota de tornozelo \"\"\"\n",
    "    # Criando nome da imagem\n",
    "    image_name = './pgm_files/image{i}.pgm'\n",
    "    # Criando imagem\n",
    "    pgm_file = open(f'./pgm_files/image{i}.pgm','w')\n",
    "    # Definindo dados do cabeçalho da imagem\n",
    "    format = 'P2'\n",
    "    height = '28'\n",
    "    width = '28'\n",
    "    max_intensity = '255'\n",
    "\n",
    "    # Escrevendo cabeçalho\n",
    "    pgm_file.write(format+'\\n')\n",
    "    pgm_file.write(f'{height} {width}'+'\\n')\n",
    "    pgm_file.write(max_intensity+'\\n')\n",
    "\n",
    "    \"\"\" Percorrendo cada linha do arquivo \n",
    "    e escrevendo o pixel e adicionando uma quebra \n",
    "    de linha a cada 28 pixels \"\"\"\n",
    "    height = 28\n",
    "    for j in range(len(img_example)):\n",
    "        pixel = str(img_example[j])\n",
    "        pgm_file.write(pixel + ' ')\n",
    "        if j == height:\n",
    "            pgm_file.write('\\n')\n",
    "            height += 28\n",
    "    \n",
    "    print(f'File ./pgm_files/image{i}.pgm created!')\n",
    "    pgm_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    read_ascii_pgm\n",
    "    Lê um arquivo PGM em formato ASCII e retorna a matriz de pixels.\n",
    "    entrada:\n",
    "        string com nome do arquivo\n",
    "    saída:\n",
    "        vetor bidimensional com dimensões 28x28 com os pixels do arquivo\n",
    "\"\"\"\n",
    "def read_ascii_pgm(filename):\n",
    "    \n",
    "    # Abrindo arquivo em modo de leitura\n",
    "    with open(filename, 'r') as f:\n",
    "        # Salavando o conteudo em uma string\n",
    "        lines = f.readlines()\n",
    "\n",
    "    \"\"\" Obtendo a largura e altura na segunda linha\n",
    "    separando-as por espaço em branco \"\"\"\n",
    "    width, height = map(int, lines[1].split())\n",
    "\n",
    "    \"\"\" Criando um vetor de dimensoes 29x29 para preencher \n",
    "    com os pixels \"\"\"\n",
    "    data = np.zeros((height+1, width+1), dtype=np.uint8)\n",
    "    # Ignorando as duas primeiras linhas (formato e dimensões)\n",
    "    for y in range(3,height):\n",
    "        # Mapeando os valores para inteiros\n",
    "        row = map(int, lines[y].split())\n",
    "        # Percorrendo o vetor e adicionando na matriz de dados\n",
    "        for x, val in enumerate(row):\n",
    "            data[y, x] = val\n",
    "    return data\n",
    "\n",
    "images = [read_ascii_pgm(f'./pgm_files/image{i}.pgm') for i in range(10)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "label=0\n",
    "\n",
    "# Iterando sobre os subplots e imagens correspondentes usando zip(axes.ravel(), images)\n",
    "# O método zip combina os elementos de axes.ravel() (subplots achatados em uma única dimensão)\n",
    "# e images (lista de imagens) em pares, permitindo iterar sobre eles ao mesmo tempo\n",
    "for ax, image in zip(axes.ravel(), images):\n",
    "    # Mostrando a imagem no subplot atual\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "    \n",
    "    # Desativando as bordas do subplot\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Definindo o título do subplot com base no dicionário de rótulos\n",
    "    ax.set_title(label_dictionary[label])\n",
    "    \n",
    "    # Incrementando o contador de rótulos\n",
    "    label += 1\n",
    "\n",
    "# Ajustando o layout da figura\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rede Neural Simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas para criação e treinamento dos modelos\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Importando bibliotecas para\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Utilizando a biblioteca TensorFlow com a API Keras para construir e treinar a rede neural\n",
    "\n",
    "\n",
    "> - A versão 1 do modelo de rede neural possui uma camada Falltten responsável por transformar dados multidimensionais em um vetor unidimensional\n",
    "> - Além disso, o modelo possui 3 camadas ocultas e uma camada de saída  \n",
    "> - A camada de entrada possui 784 nós, correspondendo ao número de pixels em cada  imagem\n",
    "> - As camadas ocultas possuem 200, 100 e 50 neurônios, respectivamente, e usam a função de ativação ReLU\n",
    "> - A camada de saída possui 10 nós e usa a função de ativação softmax para produzir uma distribuição de probabilidade de pertencimento a cada classe  \n",
    "\n",
    "\n",
    "> - O modelo é compilado utilizando o otimizador Adam com uma taxa de aprendizado de 0.001  \n",
    "> - A função de perda escolhida é a entropia cruzada categórica, apropriada para problemas de classificação multiclasse  \n",
    "> - A métrica de avaliação escolhida é a acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_modelv1 = Sequential([\n",
    "    Flatten(input_shape=(784,)),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "neural_network_modelv1.compile(optimizer=adam,\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "neural_network_modelv1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - A versão 2 do modelo possui a mesma estrutura da versão 2 mas para fins de redução de **overfitting** o modelo possui uma camada de dropout entre cada camada densa com uma taxa de dropout de 20%  \n",
    "> - O dropout é uma técnica de regularização que desativa aleatoriamente um determinado percentual de unidades de uma camada durante o treinamento, ajudando a evitar o overfitting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_modelv2 = Sequential([\n",
    "    Flatten(input_shape=(784,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "neural_network_modelv2.compile(optimizer=adam,\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "neural_network_modelv2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalização e One-Hot-Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas com os pixels\n",
    "X_train = train_fmnist.drop('label', axis=1)\n",
    "\n",
    "# Coluna com a label\n",
    "Y_train = train_fmnist['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os valores de pixel para o intervalo [0, 1]\n",
    "X_train = X_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Dividindo os dados em conjuntos de treinamento e validação (80%, 20%)  \n",
    "> - A ideia principal de dividir o conjunto de dados em um conjunto de validação é evitar que o modelo se torne bom em classificar as amostras no conjunto de treinamento, mas não seja capaz de generalizar e fazer classificações precisas nos dados que não viu antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A binarização one-hot encoding permite representar categorias como vetores numéricos, mantendo a distinção entre elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "# Aplicando one-hot encoding nas labels da base de treino\n",
    "y_train_encoded = label_binarizer.fit_transform(y_train)\n",
    "# Aplicando one-hot encoding nas labels da base de validação\n",
    "y_val_encoded = label_binarizer.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label original:\", y_train.iloc[1])\n",
    "print(\"Label após one-hot encoding:\", y_train_encoded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Treinamento da Rede Neural**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinamento da versão 1 do modelo de rede neural**\n",
    "> - O número de épocas é definido como 100, sendo assim, a rede será treinada por 100 iterações completas sobre o conjunto de dados de treinamento\n",
    "> - O tamanho do batch é definido como 500, ous seja, 500 amostras serão usadas em cada passo de treinamento\n",
    "> - O modo verbose é definido como 1 para que a saída do treinamento será mostrada durante a execução\n",
    "> - Os dados de treinamento são embaralhados (*shuffle=True*) antes de cada época para evitar que o modelo se ajuste a padrões específicos dos dados de treinamento\n",
    "> - Além disso, são fornecidos dados de validação (*x_val* e *y_val_encoded*), o que permite avaliar o desempenho do modelo em um conjunto de dados separado durante o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_historyv1 = neural_network_modelv1.fit(np.array(x_train),\n",
    "          np.array(y_train_encoded),\n",
    "          epochs=100,\n",
    "          batch_size=500,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(np.array(x_val), np.array(y_val_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinamento da versão 2 do modelo de rede neural**\n",
    "> - O callback *EarlyStopping* serve para interromper o treinamento do modelo automaticamente quando uma determinada condição não é mais atendida, com base em uma métrica monitorada. Neste caso, o callback monitora a perda (loss) no conjunto de treinamento  \n",
    "> - O parâmetro *patience* indica o número de épocas que o treinamento pode continuar sem melhoria na métrica monitorada antes que o treinamento seja interrompido, sendo assim, se a perda no conjunto de treinamento não mostrar melhoria por 5 épocas consecutivas, o treinamento será interrompido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(monitor='loss', patience=5)\n",
    "neural_network_historyv2 = neural_network_modelv2.fit(np.array(x_train),\n",
    "          np.array(y_train_encoded),\n",
    "          epochs=100,\n",
    "          batch_size=500,\n",
    "          verbose=1,\n",
    "          shuffle=True,\n",
    "          validation_data=(np.array(x_val), np.array(y_val_encoded)),\n",
    "          callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualização das métricas durante o treinamento do modelo**  \n",
    "O esperado durante o treinamento é que  \n",
    "\n",
    "- **Curva de acurácia**: deve aumentar ao longo das époas a medida que o modelo é treinado e ajustado para melhorar sua capacidade de fazer previsões corretas no conjunto de treinamento  \n",
    "- **Curva de perda**: tende a diminuir ao longo das épocas à medida que o modelo é treinado para minimizar a diferença entre as previsões e os rótulos verdadeiros no conjunto de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(10,8))\n",
    "axs[0, 0].plot(neural_network_historyv1.history['accuracy'], color='violet', label='accuracy')\n",
    "axs[0, 0].plot(neural_network_historyv1.history['val_accuracy'], color='darkviolet', label='val_accuracy')\n",
    "axs[0, 0].set_title('Neural Network Accuracy v1', fontsize=10)\n",
    "\n",
    "axs[0, 1].plot(neural_network_historyv1.history['loss'], color='orange',label='loss')\n",
    "axs[0, 1].plot(neural_network_historyv1.history['val_loss'], color='tomato', label='val_loss')\n",
    "axs[0, 1].set_title('Neural Network Loss v1', fontsize=10)\n",
    "\n",
    "axs[0, 0].legend(loc=\"upper left\")\n",
    "axs[0, 1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1, 0].plot(neural_network_historyv2.history['accuracy'], color='violet', label='accuracy')\n",
    "axs[1, 0].plot(neural_network_historyv2.history['val_accuracy'], color='darkviolet', label='val_accuracy')\n",
    "axs[1, 0].set_title('Neural Network Accuracy v2', fontsize=10)\n",
    "\n",
    "axs[1, 1].plot(neural_network_historyv2.history['loss'], color='orange',label='loss')\n",
    "axs[1, 1].plot(neural_network_historyv2.history['val_loss'], color='tomato', label='val_loss')\n",
    "axs[1, 1].set_title('Neural Network Loss v2', fontsize=10)\n",
    "\n",
    "axs[1, 0].legend(loc=\"upper left\")\n",
    "axs[1, 1].legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overfitting\n",
    "# 100 epocas\n",
    "# Early stop\n",
    "# Dropout 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Os resultados do modelo v1, demonstram que a curva de acurácia para os dados de validação se mantém constante enquanto a curva de perda de validação aumenta, isso indica um caso de **overfitting**, onde o modelo está se ajustando muito bem aos dados de treinamento, mas não está generalizando bem para dados não vistos  \n",
    "> - Sendo assim, para melhorar o desempenho da versão 1 do modelo, foram implementadas as técnicas de Dropout na modelagem e de Early Stopping para a versão 2 do modelo e os resultados obtidos foram significativamente melhores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Desempenho da Rede Neural nos Dados de Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados de teste em um DataFrame\n",
    "test_fminist = pd.read_csv('./dataset/fashion-mnist_test.csv')\n",
    "test_fminist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas com os pixels\n",
    "X_test = test_fminist.drop('label', axis=1)\n",
    "\n",
    "# Coluna com a label\n",
    "Y_test = test_fminist['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizando os valores para estarem no intervalo [0,1]\n",
    "x_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando one-hot encoding nas labels de teste\n",
    "y_test_encoded = label_binarizer.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = neural_network_modelv1.evaluate(np.array(x_test), y_test_encoded, batch_size=128)\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = neural_network_modelv2.evaluate(np.array(x_test), y_test_encoded, batch_size=128)\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Os valores de acurácia para a versão 2 do modelo foi suavemente melhor do que para a versão 1, contudo, a queda no valor de perda foi significativa o que demonstra que o modelo melhorou em termos de generalização. Para ter uma conclusão mais precisa, vamos avaliar três outras métricas:\n",
    "- Precisão \n",
    "- Recall\n",
    "- F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_test\n",
    "y_pred = neural_network_modelv1.predict(x_test)\n",
    "y_pred_discrete = np.argmax(y_pred, axis=1)\n",
    "\n",
    "precision_v1, recall_v1, f1_score_v1, _ =precision_recall_fscore_support(y_true, y_pred_discrete, average='macro')\n",
    "\n",
    "y_pred = neural_network_modelv1.predict(x_test)\n",
    "y_pred_discrete = np.argmax(y_pred, axis=1)\n",
    "precision_v2, recall_v2, f1_score_v2, _ =precision_recall_fscore_support(y_true, y_pred_discrete, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matriz de Confusão para Avaliação dos Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list(label_dictionary.values())\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator:\n",
    "  _estimator_type = ''\n",
    "  classes_=[]\n",
    "  def __init__(self, model, classes):\n",
    "    self.model = model\n",
    "    self._estimator_type = 'classifier'\n",
    "    self.classes_ = classes\n",
    "  def predict(self, X):\n",
    "    y_prob= self.model.predict(X)\n",
    "    y_pred = y_prob.argmax(axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = estimator(model, class_names)\n",
    "figsize = (9,9)\n",
    "ConfusionMatrixDisplay.from_estimator(classifier,X_test,Y_test, \n",
    "                                      cmap = 'Purples', \n",
    "                                      normalize='true', \n",
    "                                      ax=plt.subplots(figsize=figsize)[1],\n",
    "                                      display_labels=class_names,\n",
    "                                      xticks_rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rede Neural Convolucional (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas dos pixels\n",
    "X_train = train_fmnist.drop('label', axis=1)\n",
    "\n",
    "# Colunas com a label\n",
    "Y_train = train_fmnist['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn = np.array(X_train)\n",
    "x_train_cnn = x_train_cnn.reshape((x_train_cnn.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "x_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn, x_val_cnn, y_train_cnn, y_val_cnn = train_test_split(x_train_cnn, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "y_train_encoded_cnn = label_binarizer.fit_transform(y_train_cnn)\n",
    "y_val_encoded_cnn = label_binarizer.fit_transform(y_val_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A utilização de convoluções seguidas por operações de pooling permite que o modelo capture características locais e espaciais nas imagens, tornando-o adequado para tarefas de classificação de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Sequential([\n",
    "    Conv2D(32, (3,3), 1, activation='relu', \n",
    "        input_shape=(28,28,1),\n",
    "        kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(64, (3,3), 1, activation='relu',\n",
    "        kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Conv2D(128, (3,3), 1, activation='relu',\n",
    "        kernel_regularizer=l2(0.01)),\n",
    "    MaxPooling2D(),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512, activation='relu',\n",
    "          kernel_regularizer=l2(0.01)),\n",
    "    \n",
    "    Dense(10, activation='softmax',\n",
    "        kernel_regularizer=l2(0.01))\n",
    "])\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "model_cnn.compile(optimizer=adam,\n",
    "              loss=['categorical_crossentropy'],\n",
    "              metrics=['accuracy'])\n",
    "\"\"\" sgd_optimizer = SGD(learning_rate=0.001)\n",
    "model_cnn.compile(optimizer=sgd_optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy']) \"\"\"\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model_cnn, show_shapes=True, show_layer_names=True, dpi=72)\n",
    "plot_model(model_cnn, show_shapes=True, show_layer_names=True, dpi=72, to_file='./cnn_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Treinamento da Rede Neural Convolucional**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_callback = EarlyStopping(monitor='loss', patience=3)\n",
    "cnn_history = model_cnn.fit(x_train_cnn,\n",
    "            y_train_encoded_cnn,\n",
    "            epochs=100,\n",
    "            batch_size=500,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            validation_data=(x_val_cnn, y_val_encoded_cnn),\n",
    "            callbacks=[cnn_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "axs[0].plot(cnn_history.history['accuracy'], color='violet', label='accuracy')\n",
    "axs[0].plot(cnn_history.history['val_accuracy'], color='darkviolet', label='val_accuracy')\n",
    "axs[0].set_title('CNN Accuracy', fontsize=10)\n",
    "\n",
    "axs[1].plot(cnn_history.history['loss'], color='orange',label='loss')\n",
    "axs[1].plot(cnn_history.history['val_loss'], color='tomato', label='val_loss')\n",
    "axs[1].set_title('CNN Loss', fontsize=10)\n",
    "\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Desempenho da CNN nos Dados de Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cnn = np.array(x_test).reshape((x_test.shape[0], 28, 28, 1))\n",
    "y_test_encoded = label_binarizer.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_cnn.evaluate(x_test_cnn, y_test_encoded)\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matriz de Confusão para Avaliação dos Resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = estimator(model_cnn, class_names)\n",
    "figsize = (9,9)\n",
    "ConfusionMatrixDisplay.from_estimator(classifier,x_test_cnn,Y_test, \n",
    "                                      cmap = 'Purples', \n",
    "                                      normalize='true', \n",
    "                                      ax=plt.subplots(figsize=figsize)[1],\n",
    "                                      display_labels=class_names,\n",
    "                                      xticks_rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
